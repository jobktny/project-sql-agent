{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d6a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d61eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'cleaned_resources'\n",
    "from config import db_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bd2c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# define database connection\n",
    "DB_URI = db_config.DATABASE_URI()\n",
    "DATA_FOLDER = './cleaned_resources'\n",
    "\n",
    "engine = create_engine(DB_URI)\n",
    "\n",
    "# define unique key for each table\n",
    "def infer_primary_key(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].is_unique and not df[col].isnull().any():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a6ca9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf04550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_suppliers.csv supplier_id\n",
      "media_gold_reviews_chunked.csv None\n",
      "sales_customers.csv customer_id\n",
      "sales_franchises.csv franchise_id\n",
      "media_customer_reviews.csv new_id\n",
      "sales_transactions.csv transaction_id\n"
     ]
    }
   ],
   "source": [
    "# check primary key for each file\n",
    "for file in os.listdir(DATA_FOLDER):\n",
    "    df=pd.read_csv(os.path.join(DATA_FOLDER,file))\n",
    "    print(file, infer_primary_key(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_primary_key(df):\n",
    "    candidates = []\n",
    "    for col in df.columns:\n",
    "        if df[col].is_unique and not df[col].isnull().any():\n",
    "            candidates.append(col)\n",
    "    \n",
    "    if not candidates:\n",
    "        return None\n",
    "    \n",
    "    for col in candidates:\n",
    "        if \"_id\" in col.lower() or col.lower() == \"id\":\n",
    "            return col\n",
    "    return candidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30000526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Found 6 files.\n",
      "\n",
      "   ðŸ”¹ 'sales_suppliers': Found PK column 'supplier_id'.\n",
      "   âœ… Uploaded 'sales_suppliers' (48 rows)\n",
      "   âš ï¸ 'media_gold_reviews_chunked': No primary key found.\n",
      "   âœ… Uploaded 'media_gold_reviews_chunked' (196 rows)\n",
      "   ðŸ”¹ 'sales_customers': Found PK column 'customer_id'.\n",
      "   âœ… Uploaded 'sales_customers' (300 rows)\n",
      "   ðŸ”¹ 'sales_franchises': Found PK column 'franchise_id'.\n",
      "   âœ… Uploaded 'sales_franchises' (48 rows)\n",
      "   ðŸ”¹ 'media_customer_reviews': Found PK column 'new_id'.\n",
      "   âœ… Uploaded 'media_customer_reviews' (204 rows)\n",
      "   ðŸ”¹ 'sales_transactions': Found PK column 'transaction_id'.\n",
      "   âœ… Uploaded 'sales_transactions' (3333 rows)\n",
      "\n",
      "ðŸ”‘ Setting up Primary Keys...\n",
      "   âœ… Set PRIMARY KEY on sales_suppliers.supplier_id\n",
      "   âœ… Set PRIMARY KEY on sales_customers.customer_id\n",
      "   âœ… Set PRIMARY KEY on sales_franchises.franchise_id\n",
      "   âœ… Set PRIMARY KEY on media_customer_reviews.new_id\n",
      "   âœ… Set PRIMARY KEY on sales_transactions.transaction_id\n",
      "\n",
      "ðŸ”— Setting up Foreign Keys...\n",
      "   âœ… Linked media_gold_reviews_chunked.franchise_id -> sales_franchises.franchise_id\n",
      "   âœ… Linked sales_franchises.supplier_id -> sales_suppliers.supplier_id\n",
      "   âœ… Linked media_customer_reviews.franchise_id -> sales_franchises.franchise_id\n",
      "   âš ï¸ Skipping FK: sales_transactions.customer_id -> sales_customers.customer_id\n",
      "      Found 300 invalid values: [1000230, 1000054, 1000081, 1000097, 1000219, 1000035, 1000058, 1000297, 1000211, 1000277]...\n",
      "      ðŸ’¡ Fix the data in sales_transactions before adding this foreign key constraint.\n",
      "   âœ… Linked sales_transactions.franchise_id -> sales_franchises.franchise_id\n",
      "\n",
      "ðŸŽ‰ All operations completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.types import Integer, Text\n",
    "from config import db_config\n",
    "\n",
    "DATA_FOLDER = './cleaned_resources'\n",
    "\n",
    "def infer_primary_key(df):\n",
    "    \"\"\"Find the primary key column in a DataFrame\"\"\"\n",
    "    candidates = []\n",
    "    for col in df.columns:\n",
    "        if df[col].is_unique and not df[col].isnull().any():\n",
    "            candidates.append(col)\n",
    "    \n",
    "    if not candidates:\n",
    "        return None\n",
    "    \n",
    "    # Prefer columns with \"_id\" or \"id\" in the name\n",
    "    for col in candidates:\n",
    "        if \"_id\" in col.lower() or col.lower() == \"id\":\n",
    "            return col\n",
    "    return candidates[0]\n",
    "\n",
    "def validate_foreign_key_data(engine, child_table, child_col, parent_table, parent_col):\n",
    "    \"\"\"Check if all foreign key values exist in parent table\"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        # Get all distinct values in child table's FK column\n",
    "        query = f\"SELECT DISTINCT {child_col} FROM {child_table} WHERE {child_col} IS NOT NULL\"\n",
    "        child_values = pd.read_sql(query, conn)[child_col].tolist()\n",
    "        \n",
    "        # Get all values in parent table's PK column\n",
    "        query = f\"SELECT {parent_col} FROM {parent_table}\"\n",
    "        parent_values = pd.read_sql(query, conn)[parent_col].tolist()\n",
    "        \n",
    "        # Find missing values\n",
    "        missing_values = [val for val in child_values if val not in parent_values]\n",
    "        \n",
    "        return missing_values\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load all CSV files to SQL with primary keys\"\"\"\n",
    "    DB_URI = db_config.DATABASE_URI()\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    if not os.path.exists(DATA_FOLDER):\n",
    "        print(\"Creating folder...\")\n",
    "        os.makedirs(DATA_FOLDER)\n",
    "        return\n",
    "\n",
    "    csv_files = [f for f in os.listdir(DATA_FOLDER) if f.endswith('.csv')]\n",
    "    print(f\"ðŸ“‚ Found {len(csv_files)} files.\\n\")\n",
    "\n",
    "    # Store primary key info for foreign key setup\n",
    "    pk_map = {}  # {table_name: pk_column_name}\n",
    "\n",
    "    # --- Phase 1: Upload data (without PK constraint in dtype) ---\n",
    "    for file_name in csv_files:\n",
    "        table_name = os.path.splitext(file_name)[0]\n",
    "        file_path = os.path.join(DATA_FOLDER, file_name)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Find primary key\n",
    "            pk_column = infer_primary_key(df)\n",
    "            dtype_dict = {}\n",
    "\n",
    "            if pk_column:\n",
    "                # Set appropriate SQL type (but NOT primary_key=True - that's not supported in SQLAlchemy 2.0)\n",
    "                if df[pk_column].dtype in ['int64', 'int32']:\n",
    "                    dtype_dict[pk_column] = Integer()\n",
    "                else:\n",
    "                    dtype_dict[pk_column] = Text()\n",
    "                \n",
    "                pk_map[table_name] = pk_column\n",
    "                print(f\"   ðŸ”¹ '{table_name}': Found PK column '{pk_column}'.\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸ '{table_name}': No primary key found.\")\n",
    "            \n",
    "            # Upload to SQL (don't set index, keep PK as a regular column)\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                engine, \n",
    "                if_exists='replace', \n",
    "                index=False,  # Don't use index\n",
    "                dtype=dtype_dict\n",
    "            )\n",
    "            \n",
    "            print(f\"   âœ… Uploaded '{table_name}' ({len(df)} rows)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error uploading {file_name}: {e}\")\n",
    "\n",
    "    # --- Phase 2: Add Primary Key constraints using ALTER TABLE ---\n",
    "    print(\"\\nðŸ”‘ Setting up Primary Keys...\")\n",
    "    setup_primary_keys(engine, pk_map)\n",
    "\n",
    "    # --- Phase 3: Set up Foreign Keys (with validation) ---\n",
    "    print(\"\\nðŸ”— Setting up Foreign Keys...\")\n",
    "    setup_foreign_keys(engine, pk_map)\n",
    "\n",
    "    print(\"\\nðŸŽ‰ All operations completed.\")\n",
    "\n",
    "def setup_primary_keys(engine, pk_map):\n",
    "    \"\"\"Add primary key constraints using ALTER TABLE\"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        for table_name, pk_column in pk_map.items():\n",
    "            try:\n",
    "                sql = f\"\"\"\n",
    "                ALTER TABLE {table_name} \n",
    "                ADD PRIMARY KEY ({pk_column});\n",
    "                \"\"\"\n",
    "                conn.execute(text(sql))\n",
    "                conn.commit()\n",
    "                print(f\"   âœ… Set PRIMARY KEY on {table_name}.{pk_column}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ Could not set PK on {table_name}.{pk_column}: {e}\")\n",
    "\n",
    "def setup_foreign_keys(engine, pk_map):\n",
    "    \"\"\"Automatically detect and create foreign key relationships with validation\"\"\"\n",
    "    relationships = []\n",
    "    \n",
    "    # Scan all tables to find foreign keys\n",
    "    for file in os.listdir(DATA_FOLDER):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "            \n",
    "        child_table = os.path.splitext(file)[0]\n",
    "        df = pd.read_csv(os.path.join(DATA_FOLDER, file))\n",
    "        \n",
    "        # Check each column to see if it matches a primary key from another table\n",
    "        for col in df.columns:\n",
    "            for parent_table, parent_pk in pk_map.items():\n",
    "                if child_table == parent_table:\n",
    "                    continue  # Skip self-reference\n",
    "                \n",
    "                if col == parent_pk:\n",
    "                    relationships.append((child_table, col, parent_table, parent_pk))\n",
    "    \n",
    "    # Create foreign key constraints with validation\n",
    "    with engine.connect() as conn:\n",
    "        for child_table, child_col, parent_table, parent_col in relationships:\n",
    "            try:\n",
    "                # Validate data before adding constraint\n",
    "                missing_values = validate_foreign_key_data(engine, child_table, child_col, parent_table, parent_col)\n",
    "                \n",
    "                if missing_values:\n",
    "                    print(f\"   âš ï¸ Skipping FK: {child_table}.{child_col} -> {parent_table}.{parent_col}\")\n",
    "                    print(f\"      Found {len(missing_values)} invalid values: {missing_values[:10]}{'...' if len(missing_values) > 10 else ''}\")\n",
    "                    print(f\"      ðŸ’¡ Fix the data in {child_table} before adding this foreign key constraint.\")\n",
    "                    continue\n",
    "                \n",
    "                sql = f\"\"\"\n",
    "                ALTER TABLE {child_table} \n",
    "                ADD CONSTRAINT fk_{child_table}_{child_col} \n",
    "                FOREIGN KEY ({child_col}) REFERENCES {parent_table} ({parent_col});\n",
    "                \"\"\"\n",
    "                conn.execute(text(sql))\n",
    "                conn.commit()\n",
    "                print(f\"   âœ… Linked {child_table}.{child_col} -> {parent_table}.{parent_col}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ Could not link {child_table}.{child_col} -> {parent_table}.{parent_col}: {e}\")\n",
    "\n",
    "# Run the function\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be94b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Scanning for Primary Keys ---\n",
      "\n",
      "--- Step 2: Identifying Foreign Keys ---\n",
      "Table: media_gold_reviews_chunked.csv\n",
      "  â””â”€â”€ FK Column: 'franchise_id'  --> Links to: sales_franchises.csv\n",
      "------------------------------\n",
      "Table: sales_franchises.csv\n",
      "  â””â”€â”€ FK Column: 'supplier_id'  --> Links to: sales_suppliers.csv\n",
      "------------------------------\n",
      "Table: media_customer_reviews.csv\n",
      "  â””â”€â”€ FK Column: 'franchise_id'  --> Links to: sales_franchises.csv\n",
      "------------------------------\n",
      "Table: sales_transactions.csv\n",
      "  â””â”€â”€ FK Column: 'customer_id'  --> Links to: sales_customers.csv\n",
      "  â””â”€â”€ FK Column: 'franchise_id'  --> Links to: sales_franchises.csv\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# à¸ªà¸¡à¸¡à¸•à¸´à¸§à¹ˆà¸² DATA_FOLDER à¸„à¸·à¸­ path à¸‚à¸­à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¹„à¸Ÿà¸¥à¹Œ csv\n",
    "# DATA_FOLDER = 'path/to/your/csv/files'\n",
    "\n",
    "# --- Phase 1: à¸ªà¸£à¹‰à¸²à¸‡ Map à¸‚à¸­à¸‡ Primary Key à¸à¹ˆà¸­à¸™ (à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡) ---\n",
    "pk_map = {} # à¹€à¸à¹‡à¸šà¸§à¹ˆà¸²à¹„à¸Ÿà¸¥à¹Œà¹„à¸«à¸™ à¸¡à¸µ PK à¸Šà¸·à¹ˆà¸­à¸­à¸°à¹„à¸£ à¹€à¸Šà¹ˆà¸™ {'sales_customers.csv': 'customer_id'}\n",
    "\n",
    "print(\"--- Step 1: Scanning for Primary Keys ---\")\n",
    "for file in os.listdir(DATA_FOLDER):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(DATA_FOLDER, file))\n",
    "        pk = infer_primary_key(df) # à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹€à¸”à¸´à¸¡à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸¡à¸µ\n",
    "        if pk:\n",
    "            pk_map[file] = pk\n",
    "\n",
    "# --- Phase 2: à¸«à¸² Foreign Keys (à¸£à¸­à¸‡à¸£à¸±à¸šà¸«à¸¥à¸²à¸¢ column à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œà¹€à¸”à¸µà¸¢à¸§) ---\n",
    "print(\"\\n--- Step 2: Identifying Foreign Keys ---\")\n",
    "\n",
    "# à¸ªà¸£à¹‰à¸²à¸‡ dict à¹€à¸žà¸·à¹ˆà¸­à¹€à¸à¹‡à¸šà¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¹à¸šà¸šà¸ªà¸£à¸¸à¸›\n",
    "schema_relationships = {}\n",
    "\n",
    "for file in os.listdir(DATA_FOLDER):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(DATA_FOLDER, file))\n",
    "        \n",
    "        # à¸¥à¸´à¸ªà¸•à¹Œà¹„à¸§à¹‰à¹€à¸à¹‡à¸š FK à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸—à¸µà¹ˆà¹€à¸ˆà¸­à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰\n",
    "        found_fks = []\n",
    "        \n",
    "        # à¸§à¸™à¸¥à¸¹à¸›à¹€à¸Šà¹‡à¸ \"à¸—à¸¸à¸à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ\" à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰\n",
    "        for col in df.columns:\n",
    "            \n",
    "            # à¹€à¸­à¸²à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹„à¸›à¹€à¸—à¸µà¸¢à¸šà¸à¸±à¸š PK à¸‚à¸­à¸‡à¹„à¸Ÿà¸¥à¹Œà¸­à¸·à¹ˆà¸™à¹† à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”\n",
    "            for ref_file, ref_pk in pk_map.items():\n",
    "                \n",
    "                # à¸‚à¹‰à¸²à¸¡à¸–à¹‰à¸²à¹€à¸›à¹‡à¸™à¹„à¸Ÿà¸¥à¹Œà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ (à¸•à¸±à¸§à¹€à¸­à¸‡à¹€à¸—à¸µà¸¢à¸šà¸•à¸±à¸§à¹€à¸­à¸‡)\n",
    "                if file == ref_file:\n",
    "                    continue\n",
    "                \n",
    "                # à¸–à¹‰à¸²à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ à¸•à¸£à¸‡à¸à¸±à¸š PK à¸‚à¸­à¸‡à¹„à¸Ÿà¸¥à¹Œà¸­à¸·à¹ˆà¸™ = à¹€à¸ˆà¸­ FK!\n",
    "                if col == ref_pk:\n",
    "                    found_fks.append({\n",
    "                        \"fk_column\": col,\n",
    "                        \"references_table\": ref_file,\n",
    "                        \"references_pk\": ref_pk\n",
    "                    })\n",
    "        \n",
    "        # à¸–à¹‰à¸²à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰à¹€à¸ˆà¸­ FK à¸à¹‡à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸à¹‡à¸šà¹„à¸§à¹‰\n",
    "        if found_fks:\n",
    "            schema_relationships[file] = found_fks\n",
    "\n",
    "# --- à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ ---\n",
    "for table, fks in schema_relationships.items():\n",
    "    print(f\"Table: {table}\")\n",
    "    for fk in fks:\n",
    "        print(f\"  â””â”€â”€ FK Column: '{fk['fk_column']}'  --> Links to: {fk['references_table']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.types import Integer, Text\n",
    "\n",
    "# à¸ªà¸¡à¸¡à¸•à¸´à¸•à¸±à¸§à¹à¸›à¸£ connection\n",
    "DB_URI = 'postgresql+psycopg2://user:pass@localhost:5432/mydb'\n",
    "DATA_FOLDER = './data'\n",
    "\n",
    "def load_data():\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # ... (à¸ªà¹ˆà¸§à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š Folder à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡) ...\n",
    "    if not os.path.exists(DATA_FOLDER):\n",
    "        print(\"Creating folder...\")\n",
    "        os.makedirs(DATA_FOLDER)\n",
    "        return\n",
    "\n",
    "    csv_files = [f for f in os.listdir(DATA_FOLDER) if f.endswith('.csv')]\n",
    "    print(f\"ðŸ“‚ Found {len(csv_files)} files.\\n\")\n",
    "\n",
    "    # --- à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆ 1: à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸¥à¸°à¸ªà¸£à¹‰à¸²à¸‡ Primary Key ---\n",
    "    for file_name in csv_files:\n",
    "        table_name = os.path.splitext(file_name)[0]\n",
    "        file_path = os.path.join(DATA_FOLDER, file_name)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # à¹€à¸•à¸£à¸µà¸¢à¸¡ Dictionary à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸³à¸«à¸™à¸” Type à¸‚à¸­à¸‡ Database\n",
    "            dtype_dict = {}\n",
    "\n",
    "            # âœ… à¸§à¸´à¸˜à¸µà¹€à¸žà¸´à¹ˆà¸¡ PRIMARY KEY:\n",
    "            # à¸–à¹‰à¸²à¸¡à¸µà¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸Šà¸·à¹ˆà¸­ 'id' à¹€à¸£à¸²à¸ˆà¸°à¸šà¸­à¸ SQL à¸§à¹ˆà¸²à¸™à¸µà¹ˆà¸„à¸·à¸­ Integer à¹à¸¥à¸°à¹€à¸›à¹‡à¸™ Primary Key\n",
    "            if 'id' in df.columns:\n",
    "                dtype_dict['id'] = Integer(primary_key=True)\n",
    "                # à¸•à¸±à¹‰à¸‡ id à¹€à¸›à¹‡à¸™ index à¹€à¸žà¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¹ƒà¸«à¹‰ pandas à¸ªà¸£à¹‰à¸²à¸‡ index à¸‹à¹‰à¸³à¸‹à¹‰à¸­à¸™\n",
    "                df.set_index('id', inplace=True)\n",
    "                print(f\"   ðŸ”¹ '{table_name}': Set 'id' as PRIMARY KEY.\")\n",
    "            \n",
    "            # à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹€à¸‚à¹‰à¸² SQL à¸žà¸£à¹‰à¸­à¸¡à¸£à¸°à¸šà¸¸ dtype\n",
    "            df.to_sql(\n",
    "                table_name, \n",
    "                engine, \n",
    "                if_exists='replace', \n",
    "                index=True,          # à¹€à¸­à¸² index (id) à¸¥à¸‡à¹„à¸›à¸”à¹‰à¸§à¸¢\n",
    "                index_label='id',    # à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹ƒà¸™ DB à¸„à¸·à¸­ id\n",
    "                dtype=dtype_dict     # à¸ªà¹ˆà¸‡ config PK à¹€à¸‚à¹‰à¸²à¹„à¸›à¸•à¸£à¸‡à¸™à¸µà¹‰\n",
    "            )\n",
    "            \n",
    "            print(f\"   âœ… Uploaded '{table_name}' ({len(df)} rows)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error uploading {file_name}: {e}\")\n",
    "\n",
    "    # --- à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆ 2: à¸ªà¸£à¹‰à¸²à¸‡ Foreign Key (à¸—à¸³à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸„à¸£à¸šà¸—à¸¸à¸à¸•à¸²à¸£à¸²à¸‡à¹à¸¥à¹‰à¸§) ---\n",
    "    print(\"\\nðŸ”— Setting up Foreign Keys...\")\n",
    "    setup_foreign_keys(engine)\n",
    "\n",
    "    print(\"\\nðŸŽ‰ All operations completed.\")\n",
    "\n",
    "def setup_foreign_keys(engine):\n",
    "    # âœ… à¸à¸³à¸«à¸™à¸”à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸žà¸±à¸™à¸˜à¹Œà¸—à¸µà¹ˆà¸™à¸µà¹ˆ (Table à¸¥à¸¹à¸, Column à¸¥à¸¹à¸, Table à¹à¸¡à¹ˆ, Column à¹à¸¡à¹ˆ)\n",
    "    # à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: à¸•à¸²à¸£à¸²à¸‡ 'orders' à¸¡à¸µ 'user_id' à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¹„à¸›à¸«à¸² 'users.id'\n",
    "    relationships = [\n",
    "        # (\"à¸Šà¸·à¹ˆà¸­à¸•à¸²à¸£à¸²à¸‡à¸¥à¸¹à¸\", \"à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸¥à¸¹à¸\", \"à¸Šà¸·à¹ˆà¸­à¸•à¸²à¸£à¸²à¸‡à¹à¸¡à¹ˆ\", \"à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹à¸¡à¹ˆ\")\n",
    "        (\"orders\", \"user_id\", \"users\", \"id\"),\n",
    "        (\"products\", \"category_id\", \"categories\", \"id\"),\n",
    "        (\"order_items\", \"order_id\", \"orders\", \"id\"),\n",
    "    ]\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        for child_table, child_col, parent_table, parent_col in relationships:\n",
    "            try:\n",
    "                # à¸„à¸³à¸ªà¸±à¹ˆà¸‡ SQL à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸£à¹‰à¸²à¸‡ FK\n",
    "                sql = f\"\"\"\n",
    "                ALTER TABLE {child_table} \n",
    "                ADD CONSTRAINT fk_{child_table}_{child_col} \n",
    "                FOREIGN KEY ({child_col}) REFERENCES {parent_table} ({parent_col});\n",
    "                \"\"\"\n",
    "                conn.execute(text(sql))\n",
    "                conn.commit() # à¸¢à¸·à¸™à¸¢à¸±à¸™à¸„à¸³à¸ªà¸±à¹ˆà¸‡\n",
    "                print(f\"   âœ… Linked {child_table}.{child_col} -> {parent_table}.{parent_col}\")\n",
    "            except Exception as e:\n",
    "                # à¸à¸£à¸“à¸µ Error (à¹€à¸Šà¹ˆà¸™ FK à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ à¸«à¸£à¸·à¸­ à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸¡à¹ˆà¸•à¸£à¸‡à¸à¸±à¸™)\n",
    "                print(f\"   âš ï¸ Could not link {child_table} -> {parent_table}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
